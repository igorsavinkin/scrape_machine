# Scrape_machine
(EN) <br>
Soon...
--------
(RU) <br>
Программа собирает данные с сайта. Основная категория сайтов, 
где есть страницы продуктов (хранят данные). Программа начнёт работу с одной или группы страниц
и будет по ссылкам искать страницы с требуемыми данными. Чтобы данные собирались, 
требуется написание небольшого класса-инструкции. Класс следует 
создать в пакете com.scraperservice.scraper. Класс должен наследоваться 
от класса Scraper и его имя должно оканчиваться на слово Scraper (пример: LinkedinScraper.java). Реализация класса только затрагивает 
работу с html и получения данных

Примером для написания инструкции служит класс ScraperExample (его имя не соответствует правилу написания имени).
Основные классы, которые будут полезны при сборе данных:
- ScrapeUtil - общие методы для получения текста/аттрибутов/кусков кода из html страницы
- HTMLUtil - методы для работы с html. Удаления ряда тегов, аттрибутов и т.д.
- ScrapeLinkUtil - методы для работы с группой ссылок в тегах 'a' и 'img'
- TableUtil - сохранение таблиц в json

При запуске, программа попросит пройти 3 пункта:
1. Выбрать инструкцию. Инструкция будет найдена, если она соответствует всем правилам
2. Указать начальную ссылку/ссылки, откуда скрейпер начнёт работать. Можно оставить пустым. Тогда ссылка будет взята с класса-инструкции
3. Выбор того, что использовать для запросов:
   - Jsoup. Для простых get или post запросов без работы JS
   - Selenium. Использования браузера (Chrome) в headless режиме для запросов. Для работы Selenium понадобится драйвер и установленный браузер Chrome.
Драйвер скачать можно отсюда https://chromedriver.chromium.org/downloads. После, создайте папку driver в корне проекта и перенесите файл драйвера туда назвав его 'chromedriver.exe'.

На данный момент, результаты сохраняются в csv файл. Результат будет сохранён в папку results